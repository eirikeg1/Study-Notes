
# Decoder Language Models
---
_Transformer-based language models used for next-word prediction. Does not use an encoder module at all_

* Also known as **Causal Language Models** (**CLM**s), generative LMs, left-to-right LMs, GPT-like LMs or simply language models
* Goal is to estimate the probability distribution of sequences of words (or tokens in general)
* Traditional language models introduced by Clause E Shannon in 1948
	* ![[Pasted image 20240226141950.png|250]]
* Models like GPT2 started to become useful in solving certain tasks without any fine-tuning (better than random)
* GPT3 is when the model became very useful and showed a lot of **emergent abilities**, very good in zero-shot and few-shot

## Emergent abilities

* The model learns a lot of different abilities, even ones which we did not train explicitly on
* Is not a linear, or even gradual improvement, a lot of the time. Can suddenly _burst_
  * ![[Pasted image 20240226142717.png|300]]
* Is a double-edged sword for decoder language models, can be useful but is very hard to control
* Main advantage of decoder based models, otherwise they perform poorly in the pre-training/fine-tuning paradigm
	* This causes them to be huge and expensive to be somewhat useful
* In practice it might be cheaper to annotate a dataset or to infer from a huge decoder LM
	* "A small fine-tuned LM usually performs better than a huge non-fine-tuned decoder LM" (Bang et al., 2023)
* Turns out the bottleneck is not by model size (parameters), but by data


# Encoder Language models
---

* Training objective: fill in the blanks
* Alternative names: masked language models (**MLM**s), bidirectional LMs or BERT-like LMs
* Models like **BERT**
* We want to process text in both directions
* Gives strong language representation by the encoder
* Are usually better than decoder for fine-tuning
* Does not directly estimate $P(w_{0:n})$ and doesn't generate text out of the box
* MLMs work, layer by layer, similarly to a traditional NLP pipeline - first they processes the morphology, then the syntax, then the semantics
* ![[Pasted image 20240226143902.png|350]]

## BERT
_Popular Encoder-only language model which was very efficient and was the state-of-the art when it came out_

* ![[Pasted image 20240226144153.png|350]]
* Is pre-trained on two objectives at the same time: Masked language modeling (MLM) and next sentence prediction (NSP)
* The input $[CLS]\;text_{1}\dots\;[SEP]\;text_{2}\dots\;[SEP]$
* The task: predict if $text_2$ follows $test_1$ or not, using the last hidden representation of the $[CLS]$ token
* The idea is to learn a good sentence representation, not just token representations
* BERT is pre-trained on a corpus of English books and on English Wikipedia, a relatively modest amount of text in today's standards
* Was improved later with models like for example **RoBERTa**, or **DeBERTa**
	* Usually there is not one model which outperforms on all tasks, so some testing can be a good idea
* There are also multi-lingual models like **mBERT**, **XLM-R** or **NorBERT**

## ELECTRA
_Alternative training method to MLMs_

* A more efficient pre-training by using two encoders: generator and discriminator
* The generator tries to fool the discriminator
* Generator generates masked words, while discriminator tries to predict which of the words out of the generator was generated by the generator
* Difficult to set up properly in practice, as you have to balance the complexity of the models
* ![[Pasted image 20240226144929.png|350]]


# Encoder-Decoder Language Models
---
_Uses both an encoder and an decoder, based on the original "Attention is all you need"_

* Training objective: fill in the blanks, auto-regressively
* ![[Pasted image 20240226145249.png|350]]
* The training objective combines MLM and CLM together
* We mask a span of tokens, replace it with a single $\langle MASK\rangle$ token, put the text into encoder and then let decoder generate all tokens in the masked span
* Resulting LM can be used as a strong representation learner as well as a generative model
* The **T5** family of text-to-text transformers
* A rule of thumb: encoder-decoder models are worse than MLMs on fine-tuned tasks  and better than CLMs on generative tasks, when comparing models of similar scale
	* However the encoder-decoder architecture is more complicated to train and to scale up
	* Thus CLMs beat encoder-decoder models by sheer amount of parameters
* This model is more versatile, therefore more training options exist:
	* **BART** is trained with an alternative take on autoregressive MLMs
	* **UL2** further merges CLM with autoregressive MLM, leading to better zero-shot performance


$\frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$
$softmax(\frac{QK^T}{\sqrt{d_{k}}})V$
$\theta$