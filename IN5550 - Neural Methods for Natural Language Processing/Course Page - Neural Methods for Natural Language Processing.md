
# Overview
---
* This is a course where we use the PyTorch library, [[PyTorch basics|here]] is an overview of PyTorch
* [[Fox data cluster cheat sheet]]
* [Online fox notebook](https://ondemand.educloud.no/pun/sys/dashboard/batch_connect/sessions)
* [Zoom link](https://uio.zoom.us/j/62776647554?pwd=dnZ4WUMwMnExZVh2bFNIdkN3N2ZNdz09)
* [Course GitHub](https://github.uio.no/in5550/2024)
* [TensorFlow playground](playground.tensorflow.org)


# Lecture Notes
---

## Week 1
* [[Week 1 - Linear Classifiers NLP]]
* [[Week 1 - Neural Models]]

## Week 2
* [[Week 2 - Multi-layered Networks and Deep Learning]]
* [[Week 2 - Practicalities and hyper-parameters]]

## Week 3
* [[Week 3 - Count-based distributional semantic models]]
* [[Week 3 - Machine learning based distributional models]]

## Week 4
* [[Week 4 - Language Modeling]]
* [[Week 4 - Recurrent Neural Networks]]
* [[Week 4 - Contextualized embeddings]]

## Week 5
* [[Week 5 - Encoder-Decoder for Sequence2Sequence]]
* [[Week 5 - Transformers and Attention]]

## Week 6
* [[Week 6 - Transfer learning]]
* [[Week 6 - Decoder and Encoder Based Language Models]]
* [[Week 6 - Sub-word tokenization]]

## Week 7
* [[Week 7 - Fine tuning and prompting]]
* [[Week 7 - LLM Generation Strategies]]

## Week 8
* [[Week 8 - Evaluation and Benchmarking]]
* 
# Book Notes
---

* [[Chapter 1 - Introduction]]
* [[Jurafsky & Martin - Chapter 6|Chapter 6 - Vector Semantics and Embeddings]]


# Paper notes
---

* [[Rong - word2vec Parameter Learning Explained]]
* [[Vaswani et al - The Annotated Transformer]]
	* _"Attention Is All You Need", step by step walkthrough implementation in PyTorch_

# Tags
---
* #CoursePage


