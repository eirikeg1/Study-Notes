


# Predictive approaches using neural language models
---

* We try to find (predict) for each word a vector/embedding such that it is
	* Maximally similar to the vectors of its paradigmatic neighbors
	* minimally similar to the vectors of the words which in the training corpus are not paradigmatic (words uses in the same context like dog and cat) neighbors of the given word
* [[Week 6 - word2vec embeddings|word2vec embeddings]]
* ![[Pasted image 20240208192038.png|400]]

## Continuous Bag Of Words (CBOW)



## Continuous Skip-Gram (skipgram)


## fastText




# Hybrid approaches
---